{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Link:\n",
    "https://github.com/Ruhetarannum/aerial-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:51:39.388354Z",
     "iopub.status.busy": "2025-11-28T14:51:39.388151Z",
     "iopub.status.idle": "2025-11-28T14:53:24.940607Z",
     "shell.execute_reply": "2025-11-28T14:53:24.939369Z",
     "shell.execute_reply.started": "2025-11-28T14:51:39.388324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82/10\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━\u001b[0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.820m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu120m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.750m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.830m━━━\u001b[0m \u001b[32m 9/10\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy\n",
    "!pip install \"numpy<2\"\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Optional: install CPU torch & torchvision (Kaggle often has torch; this ensures CPU wheel)\n",
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:53:41.498353Z",
     "iopub.status.busy": "2025-11-28T14:53:41.497234Z",
     "iopub.status.idle": "2025-11-28T14:53:41.511701Z",
     "shell.execute_reply": "2025-11-28T14:53:41.510537Z",
     "shell.execute_reply.started": "2025-11-28T14:53:41.498314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found input folder to copy: /kaggle/input/classification-zip\n",
      "Using source folder: /kaggle/input/classification-zip/classification_dataset\n",
      "Destination already exists: /kaggle/working/dataset_copy - skipping copy (delete to force refresh).\n"
     ]
    }
   ],
   "source": [
    "# CELL 0 — copy first non-empty /kaggle/input dataset into writable /kaggle/working/dataset_copy\n",
    "import shutil, os\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "WORK_COPY = Path(\"/kaggle/working/dataset_copy\").resolve()\n",
    "\n",
    "# find first non-empty input subfolder\n",
    "cand = None\n",
    "for p in sorted(INPUT_ROOT.iterdir()):\n",
    "    try:\n",
    "        if p.is_dir() and any(p.rglob(\"*\")):\n",
    "            cand = p\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if cand is None:\n",
    "    print(\"No dataset found under /kaggle/input. If your data is already in /kaggle/working, you can skip copying.\")\n",
    "else:\n",
    "    print(\"Found input folder to copy:\", cand)\n",
    "    # prefer nested classification-like folder if obvious\n",
    "    nested = None\n",
    "    for sub in cand.rglob(\"*\"):\n",
    "        try:\n",
    "            if sub.is_dir() and any(x.name.lower() in (\"train\",\"valid\",\"val\",\"classification_dataset\") for x in sub.iterdir() if x.is_dir()):\n",
    "                nested = sub\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    src = nested if nested is not None else cand\n",
    "    print(\"Using source folder:\", src)\n",
    "    if WORK_COPY.exists():\n",
    "        print(\"Destination already exists:\", WORK_COPY, \"- skipping copy (delete to force refresh).\")\n",
    "    else:\n",
    "        print(\"Copying to writable working folder:\", WORK_COPY)\n",
    "        shutil.copytree(src, WORK_COPY, dirs_exist_ok=True)\n",
    "        (WORK_COPY / \".copied_from\").write_text(str(src))\n",
    "        print(\"Copy complete. WORK_COPY contents top-level:\", list(WORK_COPY.iterdir())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:54:44.907553Z",
     "iopub.status.busy": "2025-11-28T14:54:44.907214Z",
     "iopub.status.idle": "2025-11-28T14:54:44.918378Z",
     "shell.execute_reply": "2025-11-28T14:54:44.917147Z",
     "shell.execute_reply.started": "2025-11-28T14:54:44.907526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_ROOT: /kaggle/working/dataset_copy\n",
      ".copied_from \n",
      "test (dir)\n",
      "train (dir)\n",
      "valid (dir)\n",
      "train_dir: /kaggle/working/dataset_copy/train exists: True\n",
      "val_dir: /kaggle/working/dataset_copy/valid exists: True\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — dataset check\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Use the writable copy created by CELL 0\n",
    "WORK_ROOT = Path(\"/kaggle/working/dataset_copy/classification_dataset\")\n",
    "# Common case: dataset copied directly under /kaggle/working/dataset_copy (no extra nesting)\n",
    "if not WORK_ROOT.exists():\n",
    "    # try fallback locations under dataset_copy\n",
    "    alt1 = Path(\"/kaggle/working/dataset_copy\")\n",
    "    # prefer a folder containing 'train' and 'valid' inside dataset_copy\n",
    "    if alt1.exists():\n",
    "        for p in alt1.iterdir():\n",
    "            if p.is_dir() and (p / \"train\").exists() and (p / \"valid\").exists():\n",
    "                WORK_ROOT = p\n",
    "                break\n",
    "    # final fallback: if dataset_copy itself contains train/valid, use it\n",
    "    if (alt1 / \"train\").exists() and (alt1 / \"valid\").exists():\n",
    "        WORK_ROOT = alt1\n",
    "\n",
    "ARTIFACT_DIR = Path(\"/kaggle/working/artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT = ARTIFACT_DIR / \"classification_model.pt\"  # existing model you trained earlier\n",
    "\n",
    "print(\"WORK_ROOT:\", WORK_ROOT)\n",
    "assert WORK_ROOT.exists(), f\"WORK_ROOT not found: {WORK_ROOT}. If your dataset is under a different folder, inspect /kaggle/working/dataset_copy.\"\n",
    "\n",
    "for p in sorted(WORK_ROOT.iterdir()):\n",
    "    print(p.name, \"(dir)\" if p.is_dir() else \"\")\n",
    "\n",
    "# check train/valid presence\n",
    "train_dir = WORK_ROOT / \"train\"\n",
    "val_dir = WORK_ROOT / \"valid\"\n",
    "if not train_dir.exists():\n",
    "    # try 'train_images' or fallback to using WORK_ROOT itself\n",
    "    for alt in [\"train_images\",\"training\"]:\n",
    "        if (WORK_ROOT/alt).exists():\n",
    "            train_dir = WORK_ROOT/alt\n",
    "            break\n",
    "\n",
    "print(\"train_dir:\", train_dir, \"exists:\", train_dir.exists())\n",
    "print(\"val_dir:\", val_dir, \"exists:\", val_dir.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:55:03.810682Z",
     "iopub.status.busy": "2025-11-28T14:55:03.810416Z",
     "iopub.status.idle": "2025-11-28T14:55:10.745833Z",
     "shell.execute_reply": "2025-11-28T14:55:10.744898Z",
     "shell.execute_reply.started": "2025-11-28T14:55:03.810666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: ['bird', 'drone']\n",
      "Num classes: 2\n",
      "Train samples: 2662 Val samples: 442\n",
      "Saved /kaggle/working/artifacts/class_names.json\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — dataloaders and classes\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "WORK_ROOT = Path(\"/kaggle/working/dataset_copy/classification_dataset\")  # canonical dataset copy root\n",
    "# fallback: if above not right, try top-level copy\n",
    "if not WORK_ROOT.exists():\n",
    "    WORK_ROOT = Path(\"/kaggle/working/dataset_copy\")\n",
    "\n",
    "train_dir = WORK_ROOT / \"train\"\n",
    "val_dir = WORK_ROOT / \"valid\"\n",
    "\n",
    "# transforms (simple)\n",
    "train_tf = T.Compose([T.Resize((224,224)), T.RandomHorizontalFlip(), T.ToTensor()])\n",
    "val_tf   = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "\n",
    "# Use ImageFolder (assumes folder-per-class under train/ and valid/)\n",
    "train_ds = ImageFolder(str(train_dir), transform=train_tf)\n",
    "val_ds = ImageFolder(str(val_dir), transform=val_tf)\n",
    "\n",
    "print(\"Detected classes:\", train_ds.classes)\n",
    "print(\"Num classes:\", len(train_ds.classes))\n",
    "print(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "# Save class names to artifact for reproducibility\n",
    "import json\n",
    "ARTIFACT_DIR = Path(\"/kaggle/working/artifacts\"); ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(ARTIFACT_DIR/\"class_names.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(train_ds.classes, f)\n",
    "print(\"Saved /kaggle/working/artifacts/class_names.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:55:37.348786Z",
     "iopub.status.busy": "2025-11-28T14:55:37.348312Z",
     "iopub.status.idle": "2025-11-28T14:55:37.504982Z",
     "shell.execute_reply": "2025-11-28T14:55:37.504010Z",
     "shell.execute_reply.started": "2025-11-28T14:55:37.348769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2 class_names: ['bird', 'drone']\n",
      "No existing checkpoint found at /kaggle/working/artifacts/classification_model.pt ; proceeding from scratch.\n",
      "Model ready. Final fc shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# CELL 3 — model construction & robust checkpoint load\n",
    "import torch\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ARTIFACT_DIR = Path(\"/kaggle/working/artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT = ARTIFACT_DIR / \"classification_model.pt\"  # existing checkpoint (may have wrong fc size)\n",
    "device = \"cpu\"\n",
    "\n",
    "# number of classes detected from previous cell\n",
    "import json\n",
    "class_names = json.load(open(ARTIFACT_DIR/\"class_names.json\",\"r\",encoding=\"utf-8\"))\n",
    "num_classes = len(class_names)\n",
    "print(\"num_classes:\", num_classes, \"class_names:\", class_names)\n",
    "\n",
    "# build fresh model\n",
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Attempt to load checkpoint state dict robustly\n",
    "def extract_state_dict(maybe_dict):\n",
    "    \"\"\"If checkpoint is wrapped, try to find inner state_dict, else return as-is.\"\"\"\n",
    "    if not isinstance(maybe_dict, dict):\n",
    "        return None\n",
    "    # common keys: 'model_state_dict', 'state_dict', 'net', 'model'\n",
    "    for key in (\"model_state_dict\",\"state_dict\",\"net\",\"state\"):\n",
    "        if key in maybe_dict and isinstance(maybe_dict[key], dict):\n",
    "            return maybe_dict[key]\n",
    "    # heuristics: if values look like tensors assume it's a state dict\n",
    "    if all(isinstance(v, torch.Tensor) or (hasattr(v,'shape') and not isinstance(v, dict)) for v in maybe_dict.values()):\n",
    "        return maybe_dict\n",
    "    # try to find the largest dict-valued entry\n",
    "    for k,v in maybe_dict.items():\n",
    "        if isinstance(v, dict) and any(isinstance(x, torch.Tensor) for x in v.values()):\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "loaded = False\n",
    "if CHECKPOINT.exists():\n",
    "    ck = torch.load(CHECKPOINT, map_location=device)\n",
    "    state_dict = extract_state_dict(ck) or ck\n",
    "    # If state_dict contains fc shape mismatching, we will load with strict=False\n",
    "    try:\n",
    "        # create a temp model with fc sized like checkpoint if possible to allow better mapping\n",
    "        temp_model = models.resnet18(weights=None)\n",
    "        # if ck has 'fc.weight' we can inspect its first dimension\n",
    "        if 'fc.weight' in state_dict:\n",
    "            ck_fc_shape0 = state_dict['fc.weight'].shape[0]\n",
    "            # set temp model fc to same shape to allow exact load\n",
    "            import torch.nn as nn\n",
    "            temp_model.fc = nn.Linear(num_ftrs, ck_fc_shape0)\n",
    "            load_res = temp_model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"Loaded into temp model. Missing/unexpected keys:\", load_res)\n",
    "            # now copy backbone weights into our real model (except fc)\n",
    "            ts = temp_model.state_dict()\n",
    "            my_sd = model.state_dict()\n",
    "            for k,v in ts.items():\n",
    "                if not k.startswith(\"fc.\"):\n",
    "                    my_sd[k] = v\n",
    "            model.load_state_dict(my_sd)\n",
    "            loaded = True\n",
    "            print(\"Backbone weights copied from checkpoint (fc skipped or mismatched).\")\n",
    "        else:\n",
    "            # state_dict lacks fc key — likely backbone-only checkpoint\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            loaded = True\n",
    "            print(\"Loaded checkpoint (no fc present) with strict=False.\")\n",
    "    except Exception as e:\n",
    "        print(\"Warning: robust load failed:\", e)\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            loaded = True\n",
    "            print(\"Fallback loaded checkpoint with strict=False.\")\n",
    "        except Exception as e2:\n",
    "            print(\"Final load attempt failed — proceeding with randomly initialized model. Error:\", e2)\n",
    "else:\n",
    "    print(\"No existing checkpoint found at\", CHECKPOINT, \"; proceeding from scratch.\")\n",
    "\n",
    "# Replace final fc with new layer for correct num_classes (random init)\n",
    "import torch.nn as nn\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "model.to(device)\n",
    "print(\"Model ready. Final fc shape:\", model.fc.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:56:54.823273Z",
     "iopub.status.busy": "2025-11-28T14:56:54.821827Z",
     "iopub.status.idle": "2025-11-28T14:56:56.284654Z",
     "shell.execute_reply": "2025-11-28T14:56:56.283936Z",
     "shell.execute_reply.started": "2025-11-28T14:56:54.823217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity: train_loader size: 333 val: 28\n",
      "Model built with final fc: torch.Size([2, 512])\n",
      "Batch shapes: torch.Size([8, 3, 224, 224]) torch.Size([8])\n",
      "Loss on single batch: 0.7018393278121948\n",
      "Single-step backward completed in 1.3s\n",
      "PRELIGHT SUCCESS: data pipeline + model forward & backward OK.\n"
     ]
    }
   ],
   "source": [
    "# ---------- PRELIGHT TEST (single batch forward+backward) ----------\n",
    "import torch, time\n",
    "from pathlib import Path\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "\n",
    "# assume train_loader, val_loader, class_names were created by CELL 2\n",
    "print(\"Sanity: train_loader size:\", len(train_loader), \"val:\", len(val_loader))\n",
    "# build model same as CELL 3 but simpler: new fc with correct classes\n",
    "num_classes = len(class_names)\n",
    "model = models.resnet18(weights=None)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model built with final fc:\", model.fc.weight.shape)\n",
    "\n",
    "# try to load existing checkpoint into backbone (non-strict) if present\n",
    "ckpt_path = Path(\"/kaggle/working/artifacts/classification_model.pt\")\n",
    "if ckpt_path.exists():\n",
    "    ck = torch.load(ckpt_path, map_location=device)\n",
    "    # quick heuristic to extract nested state dict\n",
    "    if isinstance(ck, dict) and 'model_state_dict' in ck:\n",
    "        sd = ck['model_state_dict']\n",
    "    elif isinstance(ck, dict) and 'state_dict' in ck:\n",
    "        sd = ck['state_dict']\n",
    "    else:\n",
    "        sd = ck\n",
    "    try:\n",
    "        model.load_state_dict(sd, strict=False)\n",
    "        print(\"Loaded checkpoint with strict=False\")\n",
    "    except Exception as e:\n",
    "        print(\"Load warning (non-fatal):\", e)\n",
    "\n",
    "# one optimization step on one batch\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# get a single batch\n",
    "start = time.time()\n",
    "batch = next(iter(train_loader))\n",
    "imgs, labels = batch\n",
    "print(\"Batch shapes:\", imgs.shape, labels.shape)\n",
    "imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "model.train()\n",
    "opt.zero_grad()\n",
    "out = model(imgs)\n",
    "loss = criterion(out, labels)\n",
    "print(\"Loss on single batch:\", float(loss.item()))\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(\"Single-step backward completed in {:.1f}s\".format(time.time()-start))\n",
    "# Done - model is trainable on one batch\n",
    "print(\"PRELIGHT SUCCESS: data pipeline + model forward & backward OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T14:57:23.532106Z",
     "iopub.status.busy": "2025-11-28T14:57:23.531740Z",
     "iopub.status.idle": "2025-11-28T15:21:48.827331Z",
     "shell.execute_reply": "2025-11-28T15:21:48.826164Z",
     "shell.execute_reply.started": "2025-11-28T14:57:23.532080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████| 333/333 [04:34<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.5254\n",
      "Epoch 1 validation accuracy: 0.6493\n",
      "Saved new best model to /kaggle/working/artifacts/classification_model_finetuned.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████| 333/333 [04:34<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss: 0.3857\n",
      "Epoch 2 validation accuracy: 0.7534\n",
      "Saved new best model to /kaggle/working/artifacts/classification_model_finetuned.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████| 333/333 [04:39<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss: 0.3382\n",
      "Epoch 3 validation accuracy: 0.7692\n",
      "Saved new best model to /kaggle/working/artifacts/classification_model_finetuned.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████| 333/333 [04:36<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training loss: 0.2855\n",
      "Epoch 4 validation accuracy: 0.8281\n",
      "Saved new best model to /kaggle/working/artifacts/classification_model_finetuned.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 train: 100%|██████████| 333/333 [04:41<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training loss: 0.2211\n",
      "Epoch 5 validation accuracy: 0.8258\n",
      "Fine-tune done. Best val acc: 0.8280542986425339\n",
      "Best model path: /kaggle/working/artifacts/classification_model_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 — Fine-tune training loop (CPU)\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# reuse objects created earlier: model, train_loader, val_loader, class_names\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "EPOCHS = 5  \n",
    "best_val_acc = 0.0\n",
    "best_path = Path(\"/kaggle/working/artifacts/classification_model_finetuned.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch} train\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / max(1, len(train_loader))\n",
    "    print(f\"Epoch {epoch} training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            out = model(imgs)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / max(1, total)\n",
    "    print(f\"Epoch {epoch} validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(\"Saved new best model to\", best_path)\n",
    "\n",
    "print(\"Fine-tune done. Best val acc:\", best_val_acc)\n",
    "print(\"Best model path:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T15:22:14.985764Z",
     "iopub.status.busy": "2025-11-28T15:22:14.985429Z",
     "iopub.status.idle": "2025-11-28T15:22:15.201539Z",
     "shell.execute_reply": "2025-11-28T15:22:15.200339Z",
     "shell.execute_reply.started": "2025-11-28T15:22:14.985740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image: /kaggle/working/dataset_copy/valid/bird/0527b2c8cde80736_jpg.rf.e7cfd4cdde3117b1c4797fe2a669281f.jpg\n",
      "Predicted index: 0 Predicted label: bird\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 — load best model and run a single-sample prediction (prints label)\n",
    "import torch, random\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "BEST = Path(\"/kaggle/working/artifacts/classification_model_finetuned.pt\")\n",
    "if not BEST.exists():\n",
    "    BEST = Path(\"/kaggle/working/artifacts/classification_model.pt\")  # fallback\n",
    "\n",
    "# reconstruct model architecture then load\n",
    "from torchvision import models\n",
    "import json\n",
    "class_names = json.load(open(\"/kaggle/working/artifacts/class_names.json\",\"r\",encoding=\"utf-8\"))\n",
    "num_classes = len(class_names)\n",
    "model_eval = models.resnet18(weights=None)\n",
    "num_ftrs = model_eval.fc.in_features\n",
    "model_eval.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
    "model_eval.load_state_dict(torch.load(BEST, map_location=\"cpu\"))\n",
    "model_eval.eval()\n",
    "\n",
    "# find a sample image from valid\n",
    "WORK_ROOT = Path(\"/kaggle/working/dataset_copy/classification_dataset\")\n",
    "if not WORK_ROOT.exists():\n",
    "    WORK_ROOT = Path(\"/kaggle/working/dataset_copy\")\n",
    "\n",
    "sample_img = None\n",
    "for root, dirs, files in os.walk(WORK_ROOT / \"valid\"):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "            sample_img = Path(root) / f\n",
    "            break\n",
    "    if sample_img:\n",
    "        break\n",
    "\n",
    "if sample_img is None:\n",
    "    # fallback: any image\n",
    "    for root, dirs, files in os.walk(WORK_ROOT):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                sample_img = Path(root) / f\n",
    "                break\n",
    "        if sample_img:\n",
    "            break\n",
    "\n",
    "print(\"Sample image:\", sample_img)\n",
    "tf = T.Compose([T.Resize((224,224)), T.ToTensor()])\n",
    "img = tf(Image.open(sample_img).convert(\"RGB\")).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    out = model_eval(img)\n",
    "    pred = int(out.argmax(dim=1).item())\n",
    "print(\"Predicted index:\", pred, \"Predicted label:\", class_names[pred])\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8855638,
     "sourceId": 13899803,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
